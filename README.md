# Projet : Mise en ≈ìuvre d'un Data Lakehouse dirig√© par Eric Kloeckle

Ce projet consiste √† impl√©menter un pipeline complet de traitement et d'ingestion de donn√©es pour construire un Data Lakehouse. Le processus couvre l'ensemble de la cha√Æne de traitement, de la collecte de donn√©es brutes (fichiers HTML) jusqu'√† leur visualisation dans des outils de BI.

Le flux de traitement des donn√©es suit les √©tapes suivantes :
**Collecte ‚Üí Ingestion ‚Üí Extraction ‚Üí Transformation ‚Üí Analyse / Reporting**.

## üìÇ Sources des donn√©es

Les donn√©es √† la source de ce projet sont des fichiers HTML provenant des sites **LinkedIn** et **Glassdoor**. Ces fichiers, fournis dans le dossier `0_SOURCE_WEB`, contiennent trois types d'informations :

* Offres d'emploi (INFO-EMP)
* Informations sur les entreprises (INFO-SOC)
* Avis d'employ√©s sur ces entreprises (AVIS-SOC)

## üèõÔ∏è Architecture du Data Lakehouse

L'architecture du Data Lakehouse est divis√©e en plusieurs zones, chacune ayant un r√¥le d√©fini :

| Zone | R√¥le | Contenu |
| :--- | :--- | :--- |
| **`00_METADATA`** | M√©ta-donn√©es pour chaque √©tape du projet, les donn√©es "curated_zone" ont √©t√© plac√©s ici pour respecter la logique des donn√©es descriptives du projet.
| **`0_SOURCE_WEB`** | Donn√©es sources brutes | Fichiers HTML source (LinkedIn et Glassdoor) |
| **`1_LANDING_ZONE`** | Zone d'ingestion (donn√©es brutes) | Fichiers HTML copi√©s + Fichier CSV M√©tadonn√©es Techniques |
| **`3_PRODUCTION_ZONE`** | Zone analytique (REFINED) | Donn√©es structur√©es en mod√®le d√©cisionnel pour la BI |

---

## ‚öôÔ∏è Phases du projet

L'impl√©mentation est d√©compos√©e en quatre phases principales :

### Phase 0 : Recherche des sources de donn√©es
Cette phase est d√©j√† compl√©t√©e. Les donn√©es sources sont fournies dans le dossier `0_SOURCE_WEB`. Aucun travail n'est √† r√©aliser pour cette √©tape.

### Phase 1 : Ingestion (vers `1_LANDING_ZONE`)
* **Action :** Copier les fichiers HTML depuis `0_SOURCE_WEB` vers `1_LANDING_ZONE`.
* **Action :** Enregistrer simultan√©ment les ¬´ m√©tadonn√©es techniques ¬ª (date, chemins, type, etc.) dans un fichier CSV.
* **Livrables :** Scripts Python, fichiers sources copi√©s en LANDING ZONE, fichier de m√©tadonn√©es techniques.

### Phase 2 : Extraction (vers `2_CURATED_ZONE`)
* **Action :** Lire le fichier de m√©tadonn√©es techniques pour localiser et traiter chaque fichier HTML pr√©sent en LANDING ZONE.
* **Action :** Extraire les ¬´ donn√©es descriptives ¬ª (nom soci√©t√©, ville, intitul√© emploi, avis, note, etc.) contenues dans les fichiers HTML.
* **Action :** Stocker ces donn√©es extraites dans un fichier CSV de ¬´ m√©tadonn√©es descriptives ¬ª.
* **Livrables :** Script Python, fichier de m√©tadonn√©es descriptives.

### Phase 3 : ETL et Data Warehouse (vers `3_PRODUCTION_ZONE`)
* **Action :** √Ä partir du fichier de m√©tadonn√©es descriptives, utiliser un ETL (ou script Python) pour transformer, enrichir et charger les donn√©es vers la zone `3_PRODUCTION_ZONE` (REFINED ZONE).
* **Action :** Mod√©liser et impl√©menter une structure de donn√©es decisionnelle (√©toile, flocon) qui constituera le Data Warehouse.
* **Livrables :** Scripts ETL, mod√®le de donn√©es d√©cisionnel impl√©ment√©.

### Phase 4 : Reporting et Analyse
* **Action :** Concevoir des tableaux de bord BI.
* **Action :** Utiliser des outils d'analyse pour explorer et afficher les donn√©es.
* **Livrables :** Rapports d'analyses et tableaux de bord BI dynamiques.

---

## üí° Bonnes pratiques

* **Collecte non destructive :** Modification des objets sources interdite.
* **Tra√ßabilit√© :** Assur√©e via les m√©tadonn√©es techniques.
* **√âvolutivit√© :** Utilisation d'une structure "verticale" pour les fichiers de m√©tadonn√©es.
